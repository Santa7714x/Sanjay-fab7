{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santa7714x/Sanjay-fab7/blob/main/Classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwVVbXkeZPXd",
        "outputId": "91a1d117-e4b7-4cde-90fd-1b0ef4b79195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BurSMVMhV2ZF",
        "outputId": "bae65442-e118-47fc-90f5-4a07f3f7b3d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully split into train, validation, and test folders.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define source and destination paths\n",
        "source_dir = '/content/drive/MyDrive/images(STI)'\n",
        "train_dir = '/content/drive/MyDrive/train'\n",
        "val_dir = '/content/drive/MyDrive/validation'\n",
        "test_dir = '/content/drive/MyDrive/test'\n",
        "\n",
        "# Create train, validation, and test directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Split the images while ensuring each defect class is represented\n",
        "for defect_class in os.listdir(source_dir):  # Iterate through defect class folders\n",
        "    class_path = os.path.join(source_dir, defect_class)\n",
        "\n",
        "    if os.path.isdir(class_path):  # Check if it's a directory (defect class)\n",
        "        images = os.listdir(class_path)  # List images within the defect class folder\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Calculate the number of images for each split\n",
        "        train_split = int(0.8 * len(images))\n",
        "        val_split = int(0.1 * len(images))\n",
        "\n",
        "        # Select images for each split\n",
        "        train_images = images[:train_split]\n",
        "        val_images = images[train_split:train_split + val_split]\n",
        "        test_images = images[train_split + val_split:]\n",
        "\n",
        "        # Ensure directories for each defect class in train, validation, and test\n",
        "        os.makedirs(os.path.join(train_dir, defect_class), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, defect_class), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, defect_class), exist_ok=True)\n",
        "\n",
        "        # Move images to train directory\n",
        "        for img in train_images:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, defect_class, img))\n",
        "\n",
        "        # Move images to validation directory\n",
        "        for img in val_images:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, defect_class, img))\n",
        "\n",
        "        # Move images to test directory\n",
        "        for img in test_images:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(test_dir, defect_class, img))\n",
        "\n",
        "print(\"Data successfully split into train, validation, and test folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmnjh52sWB9e",
        "outputId": "4960b1c8-10c7-4870-9f22-6c591bedbf45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set paths\n",
        "train_dir = '/content/drive/MyDrive/train'\n",
        "val_dir = '/content/drive/MyDrive/validation'\n",
        "test_dir = '/content/drive/MyDrive/test'\n",
        "\n",
        "# Parameters\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Data transformations without resizing or cropping\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load data without splitting\n",
        "all_data = datasets.ImageFolder('/content/drive/MyDrive/images(STI)', transform=train_transform)\n",
        "num_classes = len(all_data.classes)\n",
        "\n",
        "# Calculate split sizes\n",
        "total_size = len(all_data)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data, test_data = random_split(all_data, [train_size, val_size, test_size])\n",
        "\n",
        "# Use DataLoader for each split\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * (224 // 8) * (224 // 8), 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel(num_classes).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize lists to store metrics for each epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Plotting Loss and Accuracy curves\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Loss curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'defect_classification_model.pth')\n",
        "print(\"Model training complete and saved as 'defect_classification_model.pth'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "111_8k-yxqwEyRzJkDbwPa3J_go9XQsGZ",
      "authorship_tag": "ABX9TyNhwtUYhSgMcm8oqgw+PU5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}